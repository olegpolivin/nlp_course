{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary prediction, episode II: make it actually work (4 points)\n",
    "\n",
    "Your main task is to use some of the tricks you've learned on the network and analyze if you can improve __validation MAE__. Try __at least 3 options__ from the list below for a passing grade. Write a short report about what you have tried. More ideas = more bonus points. \n",
    "\n",
    "__Please be serious:__ \" plot learning curves in MAE/epoch, compare models based on optimal performance, test one change at a time. You know the drill :)\n",
    "\n",
    "You can use either __pytorch__ or __tensorflow__ or any other framework (e.g. pure __keras__). Feel free to adapt the seminar code for your needs. For tensorflow version, consider `seminar_tf2.ipynb` as a starting point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Use the code from `seminar.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "!wget https://ysda-seminars.s3.eu-central-1.amazonaws.com/Train_rev1.zip\n",
    "!unzip Train_rev1.zip\n",
    "```\n",
    "I put the data on revenues into the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/Train_rev1.csv', index_col = None)\n",
    "data['Log1pSalary'] = np.log1p(data['SalaryNormalized']).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[8, 4])\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(data[\"SalaryNormalized\"], bins=20);\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(data['Log1pSalary'], bins=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>FullDescription</th>\n",
       "      <th>LocationRaw</th>\n",
       "      <th>LocationNormalized</th>\n",
       "      <th>ContractType</th>\n",
       "      <th>ContractTime</th>\n",
       "      <th>Company</th>\n",
       "      <th>Category</th>\n",
       "      <th>SalaryRaw</th>\n",
       "      <th>SalaryNormalized</th>\n",
       "      <th>SourceName</th>\n",
       "      <th>Log1pSalary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>167465</th>\n",
       "      <td>71306205</td>\n",
       "      <td>IT Infrastructure Manager</td>\n",
       "      <td>IT Infrastructure Manager, Middlesex, **** to ...</td>\n",
       "      <td>Middlesex South East</td>\n",
       "      <td>UK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>Lewis Paige</td>\n",
       "      <td>IT Jobs</td>\n",
       "      <td>From 45,000 to 55,000 per annum</td>\n",
       "      <td>50000</td>\n",
       "      <td>cwjobs.co.uk</td>\n",
       "      <td>10.819798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4279</th>\n",
       "      <td>61393029</td>\n",
       "      <td>Financial Inclusion Officer</td>\n",
       "      <td>Financial Inclusion Officer **** per annum 40 ...</td>\n",
       "      <td>Kidderminster</td>\n",
       "      <td>Kidderminster</td>\n",
       "      <td>NaN</td>\n",
       "      <td>permanent</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Property Jobs</td>\n",
       "      <td>24,186</td>\n",
       "      <td>24186</td>\n",
       "      <td>hays.co.uk</td>\n",
       "      <td>10.093571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97084</th>\n",
       "      <td>69258318</td>\n",
       "      <td>RGN / RMN Darlington</td>\n",
       "      <td>Staff Nurse RGN / RMN Darlington Up to **** pe...</td>\n",
       "      <td>Darlington, Darlington</td>\n",
       "      <td>Darlington</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Healthcare &amp; Nursing Jobs</td>\n",
       "      <td>12 to 13 per hour</td>\n",
       "      <td>24000</td>\n",
       "      <td>careworx.co.uk</td>\n",
       "      <td>10.085851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Id                        Title  \\\n",
       "167465  71306205    IT Infrastructure Manager   \n",
       "4279    61393029  Financial Inclusion Officer   \n",
       "97084   69258318         RGN / RMN Darlington   \n",
       "\n",
       "                                          FullDescription  \\\n",
       "167465  IT Infrastructure Manager, Middlesex, **** to ...   \n",
       "4279    Financial Inclusion Officer **** per annum 40 ...   \n",
       "97084   Staff Nurse RGN / RMN Darlington Up to **** pe...   \n",
       "\n",
       "                   LocationRaw LocationNormalized ContractType ContractTime  \\\n",
       "167465    Middlesex South East                 UK          NaN    permanent   \n",
       "4279             Kidderminster      Kidderminster          NaN    permanent   \n",
       "97084   Darlington, Darlington         Darlington          NaN          NaN   \n",
       "\n",
       "            Company                   Category  \\\n",
       "167465  Lewis Paige                    IT Jobs   \n",
       "4279            NaN              Property Jobs   \n",
       "97084           NaN  Healthcare & Nursing Jobs   \n",
       "\n",
       "                              SalaryRaw  SalaryNormalized      SourceName  \\\n",
       "167465  From 45,000 to 55,000 per annum             50000    cwjobs.co.uk   \n",
       "4279                             24,186             24186      hays.co.uk   \n",
       "97084                 12 to 13 per hour             24000  careworx.co.uk   \n",
       "\n",
       "        Log1pSalary  \n",
       "167465    10.819798  \n",
       "4279      10.093571  \n",
       "97084     10.085851  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_columns = [\"Title\", \"FullDescription\"]\n",
    "categorical_columns = [\"Category\", \"Company\", \"LocationNormalized\", \"ContractType\", \"ContractTime\"]\n",
    "TARGET_COLUMN = \"Log1pSalary\"\n",
    "\n",
    "data[categorical_columns] = data[categorical_columns].fillna('NaN') # cast missing values to string \"NaN\"\n",
    "\n",
    "data.sample(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Train-test split\n",
    "    \n",
    "To be completely rigorous, let's first separate data into train and validation parts before proceeding to tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size =  195814\n",
      "Validation size =  48954\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)\n",
    "data_train.index = range(len(data_train))\n",
    "data_val.index = range(len(data_val))\n",
    "\n",
    "data_train = data_train.copy()\n",
    "data_val = data_val.copy()\n",
    "\n",
    "print(\"Train size = \", len(data_train))\n",
    "print(\"Validation size = \", len(data_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing text data\n",
    "\n",
    "Just like last week, applying NLP to a problem begins from tokenization: splitting raw text into sequences of tokens (words, punctuation, etc).\n",
    "\n",
    "__Your task__ is to lowercase and tokenize all texts under `Title` and `FullDescription` columns. Store the tokenized data as a __space-separated__ string of tokens for performance reasons.\n",
    "\n",
    "It's okay to use nltk tokenizers. Assertions were designed for WordPunctTokenizer, slight deviations are okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw text:\n",
      "2         Mathematical Modeller / Simulation Analyst / O...\n",
      "100002    A successful and high achieving specialist sch...\n",
      "200002    Web Designer  HTML, CSS, JavaScript, Photoshop...\n",
      "Name: FullDescription, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw text:\")\n",
    "print(data[\"FullDescription\"][2::100000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "preprocess = lambda text: ' '.join(tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['Title'] = data_train['Title'].astype(str).apply(preprocess)\n",
    "data_val['Title'] = data_val['Title'].astype(str).apply(preprocess)\n",
    "\n",
    "data_train['FullDescription'] = data_train['FullDescription'].astype(str).apply(preprocess)\n",
    "data_val['FullDescription'] = data_val['FullDescription'].astype(str).apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized:\n",
      "2         the opportunity my client is currently seeking...\n",
      "100002    a principal railways systems engineer is requi...\n",
      "Name: FullDescription, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Tokenized:\")\n",
    "print(data_train[\"FullDescription\"][2::100000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all words are equally useful. Some of them are typos or rare words that are only present a few times. \n",
    "\n",
    "Let's count how many times is each word present in the data so that we can build a \"white list\" of known words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "token_counts = Counter()\n",
    "\n",
    "for text in data_train['Title'].values:\n",
    "    token_counts.update(text.split())\n",
    "\n",
    "for text in data_train['FullDescription'].values:\n",
    "    token_counts.update(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokens : 179241\n",
      "('and', 2123096)\n",
      "('.', 2018563)\n",
      "(',', 1853938)\n",
      "('the', 1663289)\n",
      "('to', 1613615)\n",
      "...\n",
      "('pate', 1)\n",
      "('nurseportadown', 1)\n",
      "('salesexecutivesx2northwestlondonesp', 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"Total unique tokens :\", len(token_counts))\n",
    "print('\\n'.join(map(str, token_counts.most_common(n=5))))\n",
    "print('...')\n",
    "print('\\n'.join(map(str, token_counts.most_common()[-3:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see how many words are there for each count\n",
    "plt.hist(list(token_counts.values()), range=[0, 10**4], bins=50, log=True)\n",
    "plt.xlabel(\"Word counts\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 10\n",
    "\n",
    "# tokens from token_counts keys that had at least min_count occurrences throughout the dataset\n",
    "tokens = sorted(t for t, c in token_counts.items() if c >= min_count)\n",
    "\n",
    "# Add a special tokens for unknown and empty words\n",
    "UNK, PAD = \"UNK\", \"PAD\"\n",
    "tokens = [UNK, PAD] + tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 30715\n",
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(tokens))\n",
    "assert type(tokens) == list\n",
    "assert 'me' in tokens\n",
    "assert UNK in tokens\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1.2__ Build an inverse token index: a dictionary from token(string) to it's index in `tokens` (int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_to_id = {t: i for i, t in enumerate(tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct!\n"
     ]
    }
   ],
   "source": [
    "assert isinstance(token_to_id, dict)\n",
    "assert len(token_to_id) == len(tokens)\n",
    "for tok in tokens:\n",
    "    assert tokens[token_to_id[tok]] == tok\n",
    "\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's use the vocabulary you've built to map text lines into neural network-digestible matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNK_IX, PAD_IX = map(token_to_id.get, [UNK, PAD])\n",
    "\n",
    "def as_matrix(sequences, max_len=None):\n",
    "    \"\"\" Convert a list of tokens into a matrix with padding \"\"\"\n",
    "    if isinstance(sequences[0], str):\n",
    "        sequences = list(map(str.split, sequences))\n",
    "        \n",
    "    max_len = min(max(map(len, sequences)), max_len or float('inf'))\n",
    "    \n",
    "    matrix = np.full((len(sequences), max_len), np.int32(PAD_IX))\n",
    "    for i,seq in enumerate(sequences):\n",
    "        row_ix = [token_to_id.get(word, UNK_IX) for word in seq[:max_len]]\n",
    "        matrix[i, :len(row_ix)] = row_ix\n",
    "    \n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines:\n",
      "senior support worker\n",
      "maths teacher birmingham , west midlands\n",
      "\n",
      "Matrix:\n",
      "[[24819 26849 30286     1     1     1]\n",
      " [17217 27306  3640   143 29908 17732]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lines:\")\n",
    "print('\\n'.join(data_train[\"Title\"][::100000].values), end='\\n\\n')\n",
    "print(\"Matrix:\")\n",
    "print(as_matrix(data_train[\"Title\"][::100000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's  encode the categirical data we have.\n",
    "\n",
    "As usual, we shall use one-hot encoding for simplicity. Kudos if you implement more advanced encodings: tf-idf, pseudo-time-series, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DictVectorizer(dtype=<class 'numpy.float32'>, sparse=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# we only consider top-1k most frequent companies to minimize memory usage\n",
    "top_companies, top_counts = zip(*Counter(data_train['Company']).most_common(1000))\n",
    "recognized_companies = set(top_companies)\n",
    "data_train[\"Company\"] = data_train[\"Company\"].apply(lambda comp: comp if comp in recognized_companies else \"Other\")\n",
    "\n",
    "categorical_vectorizer = DictVectorizer(dtype=np.float32, sparse=False)\n",
    "categorical_vectorizer.fit(data_train[categorical_columns].apply(dict, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The deep learning part\n",
    "\n",
    "Once we've learned to tokenize the data, let's design a machine learning experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def to_tensors(batch, device):\n",
    "    batch_tensors = dict()\n",
    "    for key, arr in batch.items():\n",
    "        if key in [\"FullDescription\", \"Title\"]:\n",
    "            batch_tensors[key] = torch.tensor(arr, device=device, dtype=torch.int64)\n",
    "        else:\n",
    "            batch_tensors[key] = torch.tensor(arr, device=device)\n",
    "    return batch_tensors\n",
    "\n",
    "def make_batch(data, max_len=None, word_dropout=0, device=torch.device('cpu')):\n",
    "    \"\"\"\n",
    "    Creates a keras-friendly dict from the batch data.\n",
    "    :param word_dropout: replaces token index with UNK_IX with this probability\n",
    "    :returns: a dict with {'title' : int64[batch, title_max_len]\n",
    "    \"\"\"\n",
    "    batch = {}\n",
    "    batch[\"Title\"] = as_matrix(data[\"Title\"].values, max_len)\n",
    "    batch[\"FullDescription\"] = as_matrix(data[\"FullDescription\"].values, max_len)\n",
    "    batch['Categorical'] = categorical_vectorizer.transform(data[categorical_columns].apply(dict, axis=1))\n",
    "    \n",
    "    if word_dropout != 0:\n",
    "        batch[\"FullDescription\"] = apply_word_dropout(batch[\"FullDescription\"], 1. - word_dropout)\n",
    "    \n",
    "    if TARGET_COLUMN in data.columns:\n",
    "        batch[TARGET_COLUMN] = data[TARGET_COLUMN].values\n",
    "    \n",
    "    return to_tensors(batch, device)\n",
    "\n",
    "def apply_word_dropout(matrix, keep_prop, replace_with=UNK_IX, pad_ix=PAD_IX,):\n",
    "    dropout_mask = np.random.choice(2, np.shape(matrix), p=[keep_prop, 1 - keep_prop])\n",
    "    dropout_mask &= matrix != pad_ix\n",
    "    return np.choose(dropout_mask, [matrix, np.full_like(matrix, replace_with)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Title': tensor([[24819, 26849, 30286,     1,     1,     1,     1],\n",
       "         [26256,   178, 17212, 17987, 13981, 20789,  3625],\n",
       "         [ 9523, 27317, 15930,    30,  7802, 26179,    58]]),\n",
       " 'FullDescription': tensor([[24819, 26849, 30286, 29617,   859, 24819, 26849, 30286, 14791, 29617],\n",
       "         [26256,   178, 17212, 17987, 13981, 20789,  3625, 22887,   792,    74],\n",
       "         [27623, 19706, 18493,  5736, 14791,  7327, 24684,   859, 27317, 15930]]),\n",
       " 'Categorical': tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " 'Log1pSalary': tensor([ 9.7115, 10.4631, 10.7144])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_batch(data_train[:3], max_len=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture\n",
    "\n",
    "Our basic model consists of three branches:\n",
    "* Title encoder\n",
    "* Description encoder\n",
    "* Categorical features encoder\n",
    "\n",
    "We will then feed all 3 branches into one common network that predicts salary.\n",
    "\n",
    "![scheme](https://github.com/yandexdataschool/nlp_course/raw/master/resources/w2_conv_arch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This clearly doesn't fit into keras' __Sequential__ interface. To build such a network, one will have to use PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a special helper module to squeeze dimensions\n",
    "class Squeezener(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalaryPredictor(nn.Module):\n",
    "    def __init__(self, n_tokens=len(tokens), n_cat_features=len(categorical_vectorizer.vocabulary_), hid_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedder = nn.Embedding(n_tokens, embedding_dim=hid_size)\n",
    "\n",
    "        self.title_encoder = nn.Sequential(\n",
    "            nn.Conv1d(hid_size, hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1),\n",
    "            Squeezener(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.description_encoder = nn.Sequential(\n",
    "            nn.Conv1d(hid_size, hid_size, kernel_size=2),\n",
    "            nn.Dropout(p=0.25),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveMaxPool1d(output_size=1),\n",
    "            Squeezener(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.categorical_encoder = nn.Sequential(\n",
    "            nn.Linear(n_cat_features, hid_size * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size * 2, hid_size * 2),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.final_predictor = nn.Sequential(\n",
    "            nn.Linear(hid_size * 4, hid_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_size, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch):\n",
    "\n",
    "        title_embeddings = self.embedder(batch['Title']).permute(0,2,1)\n",
    "#         print(title_embeddings.shape)\n",
    "        title_features = self.title_encoder(title_embeddings)\n",
    "#         print(title_features.shape)\n",
    "\n",
    "        description_embeddings = self.embedder(batch['FullDescription']).permute(0,2,1)\n",
    "#         print(description_embeddings.shape)\n",
    "        description_features = self.description_encoder(description_embeddings)\n",
    "#         print(description_features.shape)\n",
    "\n",
    "        categorical_features = self.categorical_encoder(batch['Categorical'])\n",
    "        \n",
    "        features = torch.cat([title_features, description_features, categorical_features], 1)\n",
    "        return self.final_predictor(features).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SalaryPredictor()\n",
    "batch = make_batch(data_train[:100])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "dummy_pred = model(batch)\n",
    "dummy_loss = criterion(dummy_pred, batch[TARGET_COLUMN])\n",
    "assert dummy_pred.shape == torch.Size([100])\n",
    "assert len(np.unique(dummy_pred.cpu().detach().numpy())) > 20, \"model returns suspiciously few unique outputs. Check your initialization\"\n",
    "assert dummy_loss.ndim == 0 and 0. <= dummy_loss <= 250., \"make sure you minimize MSE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation\n",
    "\n",
    "As usual, we gonna feed our monster with random minibatches of data. \n",
    "\n",
    "As we train, we want to monitor not only loss function, which is computed in log-space, but also the actual error measured in dollars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_minibatches(data, batch_size=256, shuffle=True, cycle=False, device=torch.device('cpu'), **kwargs):\n",
    "    \"\"\" iterates minibatches of data in random order \"\"\"\n",
    "    while True:\n",
    "        indices = np.arange(len(data))\n",
    "        if shuffle:\n",
    "            indices = np.random.permutation(indices)\n",
    "\n",
    "        for start in range(0, len(indices), batch_size):\n",
    "            batch = make_batch(data.iloc[indices[start : start + batch_size]], device=device, **kwargs)\n",
    "            yield batch\n",
    "        \n",
    "        if not cycle: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "We can now fit our model the usual minibatch way. The interesting part is that we train on an infinite stream of minibatches, produced by `iterate_minibatches` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "EPOCHS = 5\n",
    "DEVICE = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(model, data, batch_size=BATCH_SIZE, name=\"\", device=torch.device('cpu'), **kw):\n",
    "    squared_error = abs_error = num_samples = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in iterate_minibatches(data, batch_size=batch_size, device = device, shuffle=False, **kw):\n",
    "            batch_pred = model(batch)\n",
    "            squared_error += torch.sum(torch.square(batch_pred - batch[TARGET_COLUMN]))\n",
    "            abs_error += torch.sum(torch.abs(batch_pred - batch[TARGET_COLUMN]))\n",
    "            num_samples += len(batch_pred)\n",
    "    mse = squared_error.detach().cpu().numpy() / num_samples\n",
    "    mae = abs_error.detach().cpu().numpy() / num_samples\n",
    "    print(\"%s results:\" % (name or \"\"))\n",
    "    print(\"Mean square error: %.5f\" % mse)\n",
    "    print(\"Mean absolute error: %.5f\" % mae)\n",
    "    return mse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/userlocal/miniconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb19bb5aa5042bfa68de2944611e1e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=382.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 242.00 MiB (GPU 0; 5.80 GiB total capacity; 798.71 MiB already allocated; 168.75 MiB free; 1.02 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-684b5c79c662>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         ):\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTARGET_COLUMN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-0abf8420b55f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mdescription_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'FullDescription'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#         print(description_embeddings.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mdescription_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescription_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;31m#         print(description_features.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/nlp/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    981\u001b[0m     return (_VF.dropout_(input, p, training)\n\u001b[1;32m    982\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 983\u001b[0;31m             else _VF.dropout(input, p, training))\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 242.00 MiB (GPU 0; 5.80 GiB total capacity; 798.71 MiB already allocated; 168.75 MiB free; 1.02 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model = SalaryPredictor().to(DEVICE)\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"epoch: {epoch}\")\n",
    "    model.train()\n",
    "    for i, batch in tqdm.tqdm_notebook(enumerate(\n",
    "            iterate_minibatches(data_train, batch_size=BATCH_SIZE, device=DEVICE)),\n",
    "            total=len(data_train) // BATCH_SIZE\n",
    "        ):\n",
    "        pred = model(batch)\n",
    "        loss = criterion(pred, batch[TARGET_COLUMN])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print_metrics(model, data_val, device=DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = make_batch(data_val.iloc[0:50], device=DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    batch_pred = model(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# < A whole lot of your code > - models, charts, analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A short report\n",
    "\n",
    "Please tell us what you did and how did it work.\n",
    "\n",
    "`<YOUR_TEXT_HERE>`, i guess..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended options\n",
    "\n",
    "#### A) CNN architecture\n",
    "\n",
    "All the tricks you know about dense and convolutional neural networks apply here as well.\n",
    "* Dropout. Nuff said.\n",
    "* Batch Norm. This time it's `nn.BatchNorm*`/`L.BatchNormalization`\n",
    "* Parallel convolution layers. The idea is that you apply several nn.Conv1d to the same embeddings and concatenate output channels.\n",
    "* More layers, more neurons, ya know...\n",
    "\n",
    "\n",
    "#### B) Play with pooling\n",
    "\n",
    "There's more than one way to perform pooling:\n",
    "* Max over time (independently for each feature)\n",
    "* Average over time (excluding PAD)\n",
    "* Softmax-pooling:\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot {{e ^ {h_{i, t}}} \\over \\sum_\\tau e ^ {h_{j, \\tau}} } }$$\n",
    "\n",
    "* Attentive pooling\n",
    "$$ out_{i, t} = \\sum_t {h_{i,t} \\cdot Attn(h_t)}$$\n",
    "\n",
    ", where $$ Attn(h_t) = {{e ^ {NN_{attn}(h_t)}} \\over \\sum_\\tau e ^ {NN_{attn}(h_\\tau)}}  $$\n",
    "and $NN_{attn}$ is a dense layer.\n",
    "\n",
    "The optimal score is usually achieved by concatenating several different poolings, including several attentive pooling with different $NN_{attn}$ (aka multi-headed attention).\n",
    "\n",
    "The catch is that keras layers do not inlude those toys. You will have to [write your own keras layer](https://keras.io/layers/writing-your-own-keras-layers/). Or use pure tensorflow, it might even be easier :)\n",
    "\n",
    "#### C) Fun with words\n",
    "\n",
    "It's not always a good idea to train embeddings from scratch. Here's a few tricks:\n",
    "\n",
    "* Use a pre-trained embeddings from `gensim.downloader.load`. See last lecture.\n",
    "* Start with pre-trained embeddings, then fine-tune them with gradient descent. You may or may not download pre-trained embeddings from [here](http://nlp.stanford.edu/data/glove.6B.zip) and follow this [manual](https://keras.io/examples/nlp/pretrained_word_embeddings/) to initialize your Keras embedding layer with downloaded weights.\n",
    "* Use the same embedding matrix in title and desc vectorizer\n",
    "\n",
    "\n",
    "#### D) Going recurrent\n",
    "\n",
    "We've already learned that recurrent networks can do cool stuff in sequence modelling. Turns out, they're not useless for classification as well. With some tricks of course..\n",
    "\n",
    "* Like convolutional layers, LSTM should be pooled into a fixed-size vector with some of the poolings.\n",
    "* Since you know all the text in advance, use bidirectional RNN\n",
    "  * Run one LSTM from left to right\n",
    "  * Run another in parallel from right to left \n",
    "  * Concatenate their output sequences along unit axis (dim=-1)\n",
    "\n",
    "* It might be good idea to mix convolutions and recurrent layers differently for title and description\n",
    "\n",
    "\n",
    "#### E) Optimizing seriously\n",
    "\n",
    "* You don't necessarily need 100 epochs. Use early stopping. If you've never done this before, take a look at [early stopping callback(keras)](https://keras.io/callbacks/#earlystopping) or in [pytorch(lightning)](https://pytorch-lightning.readthedocs.io/en/latest/early_stopping.html).\n",
    "  * In short, train until you notice that validation\n",
    "  * Maintain the best-on-validation snapshot via `model.save(file_name)`\n",
    "  * Plotting learning curves is usually a good idea\n",
    "  \n",
    "Good luck! And may the force be with you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
